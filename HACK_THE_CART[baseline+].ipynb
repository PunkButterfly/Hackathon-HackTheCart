{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HACK_THE_CART[baseline+].ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! wget https://hktn2022.blob.core.windows.net/dataset/hist_data.csv\n",
        "! wget https://hktn2022.blob.core.windows.net/dataset/test.csv"
      ],
      "metadata": {
        "id": "udOB-lbkUchg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57d817ba-293b-4099-aa0c-db389fb110e3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-25 10:58:15--  https://hktn2022.blob.core.windows.net/dataset/hist_data.csv\n",
            "Resolving hktn2022.blob.core.windows.net (hktn2022.blob.core.windows.net)... failed: Name or service not known.\n",
            "wget: unable to resolve host address ‘hktn2022.blob.core.windows.net’\n",
            "--2022-05-25 10:58:16--  https://hktn2022.blob.core.windows.net/dataset/test.csv\n",
            "Resolving hktn2022.blob.core.windows.net (hktn2022.blob.core.windows.net)... failed: Name or service not known.\n",
            "wget: unable to resolve host address ‘hktn2022.blob.core.windows.net’\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WV6F-yzyUYpl"
      },
      "outputs": [],
      "source": [
        "# Пример решения с использованием статистического подхода - подсчет \n",
        "# совстречаемостей.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gc\n",
        "\n",
        "from collections import Counter \n",
        "\n",
        "hist_data = pd.read_csv('hist_data.csv')\n",
        "\n",
        "# соберем словарь встречаемостей - какие item_id покупались чаще с \n",
        "# каждым item_id \n",
        "tmp = (\n",
        "    hist_data[['item_id', 'pav_order_id']]\n",
        "    .sort_values(['item_id', 'pav_order_id'])\n",
        "    .merge(hist_data[['item_id', 'pav_order_id']], how='left', on=['pav_order_id'], suffixes=('', '_left'))\n",
        ")\n",
        "tmp = tmp[tmp['item_id'] != tmp['item_id_left']].copy()\n",
        "tmp1 = tmp.groupby(['item_id'])['item_id_left'].agg(lambda x: Counter(x).most_common(10))\n",
        "\n",
        "most_freq_dict = {k: v for (k, v) in tmp1.iteritems()}\n",
        "\n",
        "del tmp1, tmp\n",
        "gc.collect()\n",
        "\n",
        "test = pd.read_csv('test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "grouped = hist_data.groupby(by = 'buyer_id')['pav_order_id'].value_counts()\n",
        "user_order_count = [0 for i in range(63925)]\n",
        "it = 0\n",
        "for x in hist_data['buyer_id'].unique():\n",
        "  user_order_count[it] = (x, len(grouped[x]))\n",
        "  it += 1\n",
        "user_order_count = sorted(user_order_count, key = lambda x: x[1], reverse=True)"
      ],
      "metadata": {
        "id": "ObMXAg-6Umil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "A8HcsEJrYSGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fucking_slaves = defaultdict(int)\n",
        "for user in user_order_count:\n",
        "  if user[1] > 2:\n",
        "    fucking_slaves[user[0]] = 1"
      ],
      "metadata": {
        "id": "YOyivzbvXu-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from math import ceil\n",
        "def get_rate(user_id):\n",
        "  buys = hist_data[hist_data['buyer_id'] == user_id]['item_id'].value_counts()\n",
        "  max_val = max(buys)\n",
        "  min_val = min(buys)\n",
        "  buys = buys*10/max_val\n",
        "    \n",
        "  return buys"
      ],
      "metadata": {
        "id": "NvmKJ3ZQVPsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# из списка кандидатов по совстречаемости удаляем повторяющиеся item_id, \n",
        "# сохраняя порядок\n",
        "def get_unique_recs(recs: list, top_n: int) -> list:\n",
        "    rec_dict = {}\n",
        "    counter = 0\n",
        "    for k, v in recs:\n",
        "        if k not in rec_dict:\n",
        "            rec_dict[k] = v\n",
        "            counter += 1\n",
        "        if counter == top_n:\n",
        "            break\n",
        "    return list(rec_dict.keys())\n",
        "\n",
        "def rec_by_item(item_id: int, most_freq_dict: dict) -> list:\n",
        "    \n",
        "    return most_freq_dict.get(item_id, None)\n",
        "\n",
        "# для каждого item_id соберем top_n самых часто встречающихся item_id, \n",
        "# отсортируем по частоте и выберем уникальные\n",
        "def rec_by_basket(buyer_id : int, basket: list, most_freq_dict: dict, top_n: int = 20) -> list:\n",
        "    \n",
        "    if buyer_id in fucking_slaves:\n",
        "      most_rated = list(get_rate(buyer_id).index)\n",
        "      reccomendation = []\n",
        "      for item in most_rated:\n",
        "        if item not in basket:\n",
        "          reccomendation.append(item)\n",
        "        if len(reccomendation) == top_n: #TODO: поменьять на top_n//2\n",
        "          break\n",
        "      if len(reccomendation) < top_n:\n",
        "        print(\"ПИЗДА\", len(reccomendation))\n",
        "      return reccomendation\n",
        "\n",
        "    res = []\n",
        "    for item in basket:\n",
        "        recs = rec_by_item(item, most_freq_dict)\n",
        "        if recs is not None:\n",
        "            res += recs\n",
        "    \n",
        "    res = sorted(res, key=lambda x: x[1], reverse=True)\n",
        "    \n",
        "    return get_unique_recs(res, top_n)"
      ],
      "metadata": {
        "id": "tbWTNywXVo4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def split_data(data, test_size=0.3):\n",
        "    orders_sort = data[['pav_order_id', 'created']].drop_duplicates().sort_values(by=['created', 'pav_order_id'])\n",
        "    train_orders, test_orders = train_test_split(orders_sort['pav_order_id'].tolist(), test_size=test_size, shuffle=False)\n",
        "    train_orders, test_orders = set(train_orders), set(test_orders)\n",
        "    train = data[data['pav_order_id'].apply(lambda x: x in train_orders)]\n",
        "    test = data[data['pav_order_id'].apply(lambda x: x in test_orders)]\n",
        "    return train, test, orders_sort, train_orders, test_orders\n",
        "    \n",
        "def dcg(\n",
        "    y_relevance: np.ndarray\n",
        ") -> float:\n",
        "    return np.sum([(2**i - 1) / np.log2(k + 1) for (k, i) in enumerate(y_relevance, start=1)])\n",
        "\n",
        "def ndcg(\n",
        "    y_relevance: np.ndarray,\n",
        "    k: int\n",
        ") -> float:\n",
        "    if y_relevance.sum() == 0:\n",
        "        return 0.0\n",
        "    DCG = dcg(y_relevance[:k])\n",
        "    IDCG = dcg(-np.sort(-y_relevance)[:k])\n",
        "    return DCG / IDCG\n",
        "\n",
        "def apply_relevance(x):\n",
        "    return [int(item in x['basket']) for item in x['preds']]\n",
        "\n",
        "def create_relevance(pred):\n",
        "    d = pred.copy()\n",
        "    d['basket'] = d['basket'].apply(set)\n",
        "    d = d.apply(apply_relevance, axis=1)\n",
        "    return d\n",
        "\n",
        "def ndcg_full_dataset(d):\n",
        "    dd = pd.DataFrame(d.to_list()).fillna(0).to_numpy()\n",
        "    k = dd.shape[1]\n",
        "    scores = [ndcg(dd[i], k) for i in range(len(dd))]\n",
        "    return np.mean(scores)\n",
        "\n",
        "def compute_ndcg_score(pred):\n",
        "    relevance = create_relevance(pred)\n",
        "    return ndcg_full_dataset(relevance)\n",
        "\n",
        "def make_coocurs_dict(train_data):\n",
        "    tmp = (\n",
        "        train_data[['item_id', 'pav_order_id']]\n",
        "        .sort_values(['item_id', 'pav_order_id'])\n",
        "        .merge(train_data[['item_id', 'pav_order_id']], how='left', on=['pav_order_id'], suffixes=('', '_left'))\n",
        "    )\n",
        "    tmp = tmp[tmp['item_id'] != tmp['item_id_left']].copy()\n",
        "    tmp1 = tmp.groupby(['item_id'])['item_id_left'].agg(lambda x: Counter(x).most_common(10))\n",
        "\n",
        "    most_freq_dict = {k: v for (k, v) in tmp1.iteritems()}\n",
        "\n",
        "    del tmp1, tmp\n",
        "    gc.collect()\n",
        "    return most_freq_dict\n",
        "\n",
        "def create_basket(test_data):\n",
        "    pred = test_data.groupby(['pav_order_id'])['item_id'].agg([('basket', list)])\n",
        "    return pred\n",
        "\n",
        "def make_predictions(test_data, most_freq_dict):\n",
        "    pred = test.groupby(['pav_order_id', 'buyer_id'])['item_id'].agg([('basket', list)])\n",
        "    pred.reset_index(inplace=True)\n",
        "    xuy1 = []\n",
        "    xuy2 = []\n",
        "    for index, row in pred.iterrows():\n",
        "      order_id = row['pav_order_id']\n",
        "      preds = rec_by_basket(row['buyer_id'], row['basket'], most_freq_dict=most_freq_dict)\n",
        "      xuy1.append(order_id)\n",
        "      xuy2.append(preds)\n",
        "    ans = pd.DataFrame({'pav_order_id': xuy1, 'preds': xuy2})\n",
        "    return pred"
      ],
      "metadata": {
        "id": "N6673L5ZkYGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# считываем исторические данные\n",
        "data = pd.read_csv(\"hist_data.csv\", parse_dates=['created'])\n",
        "\n",
        "# разобьем историю в отношении 70 на 30 для трейна и валидации\n",
        "train_data, test_data, orders_sort, train_orders, test_orders = split_data(data)\n",
        "\n",
        "# соберем словарь встречаемостей - какие item_id покупались чаще с каждым item_id \n",
        "most_freq_dict = make_coocurs_dict(train_data)\n",
        "# предсказываем\n",
        "pred = make_predictions(test_data, most_freq_dict)\n",
        "pred.to_csv(\"preds_on_splitted_hist_data.csv\")\n",
        "\n"
      ],
      "metadata": {
        "id": "NjzhHhcz1d3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# посчитаем скор для всего набора предсказаний\n",
        "d_score = compute_ndcg_score(pred)\n",
        "print(d_score)"
      ],
      "metadata": {
        "id": "bqCnzpfN1ios"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bLjoI-E73RoF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}